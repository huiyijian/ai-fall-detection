# 🏥 交互式老人跌倒检测与AI急救助手 - 项目计划书 (v3.0)

> **核心变更**：从“单向报警”升级为 **“双向语音交互”**。
> **场景定义**：本地 PC 仿真环境 (Python 3.10) + 视频流模拟实时监控 + YOLO 纯视觉检测 + VLM 场景诊断 + TTS/STT 语音安抚。

---

## 1. 项目概述 (Overview)

本项目旨在构建一个**具备“人文关怀”的智能跌倒检测系统**。区别于传统系统冷冰冰的报警，本系统在检测到跌倒后，会化身为“AI急救助手”，通过语音与老人对话，评估伤情并给予心理慰藉，真正实现**“检测+诊断+交互”**的全流程闭环。

### 1.1 开发环境规范
*   **Python 版本**: `3.10` (兼容性最佳，推荐使用 `conda create -n ai_fall python=3.10` 或现有环境)。
*   **硬件平台**: 本地 Windows PC (利用高性能 CPU/GPU 进行仿真)。
*   **输入源**: 本地视频文件 (模拟实时摄像头流，支持随时中断/控制)。

---

## 2. 系统架构与技术栈 (Architecture & Tech Stack)

### 2.1 模块划分

| 模块名称 | 核心技术 | 功能描述 | 关键约束 |
| :--- | :--- | :--- | :--- |
| **视频流仿真层** | `OpenCV` | 模拟摄像头行为。逐帧读取视频文件，支持 `q` 键随时退出，**绝不阻塞**。 | **严禁等待视频播放结束**。必须是流式处理 (While Loop)。 |
| **跌倒检测层** | `YOLOv8-Pose` | **纯视觉**判断。利用骨骼关键点 (Keypoints) 和边界框长宽比判定跌倒。 | **禁止使用大模型检测**。保证毫秒级响应 (Real-time)。 |
| **认知诊断层** | `Qwen-VL (QVQ)` | **视觉理解**。接收跌倒截图，分析场景危险度 (速度/地面/姿态)。 | **Prompt 必须场景化** (Input: 图; Output: 急救建议)。 |
| **语音交互层** | `pyttsx3` (TTS) <br> `SpeechRecognition` (STT) | **双向沟通**。TTS 播报建议，STT 监听老人反馈 ("我腿疼" / "起不来")。 | 需处理并发，避免语音播报时卡死视频画面。 |

---

## 3. 详细实施方案 (Implementation Details)

### 3.1 视频流模拟与控制 (解决“卡死”痛点)
*   **痛点**：之前的代码往往使用 `for frame in result` 或者等待视频播完才处理逻辑。
*   **解决方案**：
    *   采用 **“帧生产者”** 模式。
    *   主循环：`while True: ret, frame = cap.read()`。
    *   **中断机制**：检测到按键 `cv2.waitKey(1)` 立即 `break`，或检测到“跌倒确认”后，**主动暂停**视频流，进入“急救交互模式”。
    *   **循环推流 (可选)**：视频播完后自动 `set(cv2.CAP_PROP_POS_FRAMES, 0)` 重播，模拟永久监控。

### 3.2 跌倒检测逻辑 (YOLO Only)
*   **步骤**：
    1.  **Pose Estimation**: 获取 肩 (`5,6`), 髋 (`11,12`), 膝 (`13,14`), 踝 (`15,16`) 坐标。
    2.  **几何规则**:
        *   **宽高比 (Aspect Ratio)**: `bbox_width / bbox_height > 1.2` (躺平)。
        *   **躯干角度 (Torso Angle)**: 脊柱连线与垂直线夹角 `> 60度`。
    3.  **时序确认**: 连续 `N` 帧满足条件 -> 触发 `FALL_DETECTED` 事件。

### 3.3 大模型认知与提示词工程 (Prompt Engineering)
*   **触发时机**: `FALL_DETECTED` 事件确立后，截取当前帧。
*   **角色设定**: "你是一名专业的急救医生，正在通过监控观察一位刚跌倒的老人。"
*   **Prompt 模板**:
    > "分析这张图片。
    > 1. **跌倒姿态**: 是侧身倒地、面部朝下还是仰面？头部是否着地？
    > 2. **环境风险**: 地面是否湿滑？周围有无尖锐家具？
    > 3. **行动建议**: 基于上述分析，给出一段**口语化**的简短建议（用于语音播报）。
    >    *   如果看起来严重（如头部着地），请说：'请不要乱动，我已通知家属，请保持平躺。'
    >    *   如果看起来轻微，请说：'您还好吗？如果感觉疼痛请不要强行站起，告诉我哪里不舒服？'"
*   **输出要求**: JSON 格式或纯文本，方便 TTS 直接朗读。

### 3.4 语音交互循环 (The Interaction Loop)
1.  **System (TTS)**: "检测到您跌倒了，请问您现在感觉怎么样？" (由 VLM 生成或预设)
2.  **User (STT)**: 监听麦克风 5秒... "我的腿很疼，起不来。"
3.  **System (VLM)**: 将 `{"image_context": "fall", "user_audio": "leg hurts"}` 发回大模型。
4.  **System (TTS)**: "收到，腿部疼痛请不要移动，防止二次伤害。救护车已在路上..."

---

## 4. 可行性分析与关键卡点 (Feasibility & Bottlenecks)

### 4.1 关键卡点
1.  **API 延迟**: 大模型分析+生成语音可能需要 3-5 秒。
    *   *对策*: 在等待大模型时，先播放**本地预设**的安抚语音 ("正在分析伤情，请保持冷静...")，填补静默期。
2.  **误触发**: 蹲下系鞋带被误判为跌倒。
    *   *对策*: 增加 **"起身检测"**。如果跌倒后 3秒内 重新站起 (Y坐标回升)，则取消报警。
3.  **音频干扰**: 视频本身的背景音可能会干扰 STT。
    *   *对策*: 进入交互模式时，**静音** 视频播放，只开启麦克风。

### 4.2 汇报展示策略 (High Score Hacking)
*   **展示“思考过程”**: 在界面侧边栏打印大模型的“思维链” (CoT) —— "识别到头部未着地 -> 判定风险中等 -> 决定询问感受"。
*   **展示“实时性”**: 在视频画面左上角显示 `FPS: 30+`，证明 YOLO 跑得很快。
*   **展示“人性化”**: 现场模拟对话，展示系统如何根据老人的回答改变建议（这是最大的加分项）。

---

## 5. 立即执行计划 (Action Plan)

1.  **环境配置**: 完善 `requirements.txt`，补充语音相关库。
2.  **原型开发**:
    *   Step 1: 写好视频流播放器 (支持键盘控制)。
    *   Step 2: 接入 YOLO 实现跌倒红框。
    *   Step 3: 接入 TTS 念出固定台词。
    *   Step 4: 接入 VLM 替换固定台词。

---
*Created by Trae AI Pair Programmer*
