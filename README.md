# 🏥 交互式老人跌倒检测与AI急救助手

> **基于边缘计算与多模态大模型的智能跌倒检测系统**  
> 结合 YOLOv8-Pose 实时检测 + Qwen/QVQ-72B 视觉理解 + TTS 语音交互

---

## 📋 项目简介

随着全球老龄化趋势加剧，空巢老人和独居老人的安全问题日益凸显。本项目旨在开发一套**非接触式、智能化**的跌倒检测与急救响应系统。

### 核心特色

- **非接触式检测**: 利用计算机视觉技术，老人无需佩戴任何设备
- **端云协同架构**: 边缘端毫秒级检测 (YOLO) + 云端深度认知 (QVQ)
- **智能交互**: 跌倒后系统自动分析场景并生成针对性急救建议
- **语音安抚**: 使用女性 TTS 语音播报，提供人性化的紧急指导

---

## 🏗️ 系统架构

```
┌─────────────────────────────────────────────────────────────┐
│                     边缘端 (Edge)                        │
│  ┌──────────────┐      ┌──────────────┐                 │
│  │  视频采集    │──────▶│ YOLOv8-Pose │                 │
│  │ (本地视频/    │      │  跌倒检测   │                 │
│  │  摄像头)      │      └──────┬───────┘                 │
│  └──────────────┘             │                          │
│                               ▼                          │
│                         ┌───────────┐                     │
│                         │ 时序滤波   │                     │
│                         │ 防误报    │                     │
│                         └──────┬────┘                     │
└───────────────────────────────┼──────────────────────────────┘
                                │ API 请求
                                ▼
┌─────────────────────────────────────────────────────────────┐
│                   云端 (Cloud)                           │
│  ┌─────────────────────────────────────────────────────┐   │
│  │      Qwen/QVQ-72B-Preview (视觉大模型)         │   │
│  │   • 场景理解 (跌倒姿态、环境风险)              │   │
│  │   • 急救建议生成 (JSON 格式)                   │   │
│  └───────────────────┬─────────────────────────────┘   │
│                      │ 建议文本                       │
└──────────────────────┼───────────────────────────────────┘
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                   用户交互层                             │
│  ┌──────────────┐      ┌──────────────┐                 │
│  │  TTS 语音播报 │◀─────│ 交互模式    │                 │
│  │  (女性声音)   │      │ (STT 待实现) │                 │
│  └──────────────┘      └──────────────┘                 │
└─────────────────────────────────────────────────────────────┘
```

---

## 🛠️ 技术栈

### 视觉检测
| 技术 | 版本 | 用途 |
|------|------|------|
| YOLOv8-Pose | v8.0+ | 人体关键点检测 |
| OpenCV | 4.8+ | 视频流处理、显示 |
| NumPy | 1.24+ | 数值计算 |

### 认知与交互
| 技术 | 版本 | 用途 |
|------|------|------|
| Qwen/QVQ-72B-Preview | - | 多模态视觉大模型 |
| OpenAI SDK | 1.0+ | API 调用封装 |
| pyttsx3 | 2.90+ | 文字转语音 (TTS) |

### 工程化
| 技术 | 用途 |
|------|------|
| Threading | 异步 TTS 播报，不阻塞视频流 |
| Base64 | 图像编码传输 |

---

## 📦 环境配置

### 系统要求
- **Python**: 3.8 - 3.10 (推荐 3.10)
- **操作系统**: Windows / Linux / macOS
- **硬件**: NVIDIA GPU (可选，用于加速)

### 安装步骤

```bash
# 1. 克隆仓库
git clone https://github.com/huiyijian/ai-fall-detection.git
cd ai-fall-detection

# 2. 创建虚拟环境 (推荐)
conda create -n ai_fall python=3.10
conda activate ai_fall

# 3. 安装依赖
pip install -r requirements.txt

# 4. 下载 YOLOv8-Pose 模型 (首次运行自动下载)
# 模型文件: yolov8n-pose.pt
```

### 依赖清单

```
# Core Vision & AI
ultralytics>=8.0.0
opencv-python>=4.8.0
numpy>=1.24.0

# API & Networking
openai>=1.0.0
requests>=2.31.0

# Audio & Interaction
pyttsx3>=2.90  # Text-to-Speech
SpeechRecognition>=3.10.0  # Speech-to-Text (待实现)
pyaudio>=0.2.13  # Microphone access

# Utils
pillow>=10.0.0
python-dotenv>=1.0.0
```

---

## 🚀 快速开始

### 1. 配置 API Key

在 `main.py` 中修改 API Key：

```python
vlm_analyzer = VLMAnalyzer(
    api_key="your-api-key-here"  # 替换为你的 ModelScope API Key
)
```

### 2. 运行系统

```bash
python main.py
```

### 3. 操作指南

| 按键 | 功能 |
|------|------|
| `q` | 退出程序 |
| `s` | 暂停/继续 (仅视频模式) |
| `e` | 手动进入/退出交互模式 |

---

## 📁 项目结构

```
ai-fall-detection/
├── main.py                  # 主程序入口
├── vlm_analyzer.py          # 视觉大模型分析器
├── requirements.txt          # 依赖清单
├── .gitignore              # Git 忽略规则
├── README.md               # 项目文档 (本文件)
├── project_plan.md         # 原始计划书
├── project_plan_v2_local_pc.md  # 本地开发计划
├── project_plan_v3_interactive_demo.md  # 交互式演示计划
├── project_plan_v4_mvp.md  # MVP 开发计划 (当前)
├── *.mp4                  # 测试视频文件
└── yolov8n-pose.pt        # YOLO 模型 (首次运行自动下载)
```

---

## ✅ 开发进度

### MVP 阶段 (当前)

| 模块 | 功能 | 状态 |
|------|------|------|
| 视频流模拟 | 本地视频循环播放，支持键盘控制 | ✅ 已完成 |
| 跌倒检测 | YOLOv8-Pose + 几何规则判定 | ✅ 已完成 |
| 时序滤波 | 连续 N 帧确认跌倒 | ✅ 已完成 |
| 冷却机制 | 30 秒防重复报警 | ✅ 已完成 |
| VLM 分析 | Qwen/QVQ-72B 场景理解 | ✅ 已完成 |
| TTS 语音 | 女性声音播报 | ✅ 已完成 |
| 交互模式 | 跌倒后紧急对话 | ✅ 已完成 |
| 实时摄像头 | 支持 `video_path = 0` | ✅ 已完成 (代码已支持) |

### 后续迭代计划 (Nice-to-Have)

| 优先级 | 功能 | 预计时间 | 汇报价值 |
|--------|------|----------|----------|
| P0 | Gradio 可视化 Dashboard | 2h | ⭐⭐⭐⭐⭐ |
| P1 | YOLOv8 CUDA 加速 | 1h | ⭐⭐⭐⭐ |
| P2 | 卡尔曼滤波 (关键点平滑) | 1.5h | ⭐⭐⭐⭐ |
| P3 | 多帧上传 + 跌倒速度分析 | 1h | ⭐⭐⭐ |
| P4 | 本地 STT (Whisper tiny) | 0.5h | ⭐⭐ |
| P5 | Orange Pi 部署 (硬件移植) | 待定 | ⭐⭐⭐ |

详细计划请参考: [project_plan_v4_mvp.md](./project_plan_v4_mvp.md)

---

## 🎯 核心功能说明

### 1. 跌倒检测算法

**判断依据**:
- **宽高比**: `bbox_width / bbox_height > 1.2` (躺平判定)
- **躯干角度**: 肩部-髋部连线与垂直线夹角 `> 60°`

**防误报机制**:
- **时序滤波**: 连续 5 帧检测到跌倒才触发报警
- **冷却机制**: 30 秒内不重复报警

### 2. 视觉大模型 Prompt

```python
"""
你是一名专业的急救医生，正在通过监控观察一位刚跌倒的老人。
你的任务是分析图片并给出针对性的急救建议。

请按照以下格式输出：
1. 跌倒姿态分析：描述老人是侧身倒地、面部朝下还是仰面，头部是否着地
2. 环境风险评估：判断地面是否湿滑、周围有无尖锐物体
3. 急救建议：用口语化、简短的话语给出建议（用于语音播报）

建议应该根据严重程度区分：
- 严重（头部着地/撞击剧烈）：强调不要移动，保持平躺
- 中等：询问意识状态，检查是否有剧痛
- 轻微：提醒缓慢起身，注意平衡
"""
```

### 3. 异步 TTS 播报

使用独立线程进行语音播报，确保不阻塞视频流：

```python
def speak_async(self, text):
    thread = threading.Thread(target=self.speak, args=(text, True))
    thread.start()
```

---

## 📊 演示效果

### 系统界面

```
┌─────────────────────────────────────────────────────────────┐
│  FPS: 30                                               │
│                                                         │
│  ┌──────────────────────┐                                │
│  │                      │  [NORMAL]                      │
│  │       人物检测框      │  宽高比: 0.8                  │
│  │                      │  躯干角度: 15°                │
│  └──────────────────────┘                                │
└─────────────────────────────────────────────────────────────┘
```

### 跌倒触发流程

1. **检测阶段**: 连续 5 帧判定为跌倒 → 显示红色边框
2. **分析阶段**: 截取当前帧 → 调用 QVQ API
3. **交互阶段**: 
   - 显示 "EMERGENCY MODE ACTIVE"
   - TTS 播报急救建议
   - 终端打印 VLM 分析结果
4. **恢复阶段**: 100 帧后自动退出交互模式

---

## 🔧 故障排查

### 常见问题

| 问题 | 解决方案 |
|------|----------|
| `ModuleNotFoundError: No module named 'ultralytics'` | `pip install -r requirements.txt` |
| `TTS 初始化失败` | 检查系统是否安装了 TTS 引擎 (Windows 默认支持) |
| `VLM 分析失败` | 检查 API Key 是否正确，网络是否通畅 |
| `视频无法打开` | 检查视频文件路径是否正确 |

### 调试模式

在 `main.py` 中修改以下参数进行调试：

```python
# 降低跌倒判定阈值
fall_threshold_frames = 3  # 默认 5

# 缩短冷却时间
cooldown_seconds = 10  # 默认 30
```

---

## 🎓 汇报建议 (For High Score)

### 核心亮点

1. **端云协同架构**: 边缘端实时检测 + 云端深度理解
2. **工程化能力**: 异步 TTS、时序滤波、冷却机制
3. **Prompt 工程**: 场景化急救医生角色设定
4. **可扩展性**: 代码模块化，易于移植到 Orange Pi

### 汇报流程

1. **项目背景**: 老龄化 + 跌倒危害
2. **技术方案**: YOLOv8 + QVQ + TTS
3. **演示视频**: 跌倒检测 → 报警 → 语音建议
4. **技术深度**: 时序滤波、异步架构、Prompt 调优
5. **后续优化**: 卡尔曼滤波、CUDA 加速、Gradio Dashboard
6. **总结与展望**

---

## 📄 许可证

MIT License

---

## 🤝 贡献

欢迎提交 Issue 和 Pull Request！

---

## 👨‍💻 作者

Trae AI Pair Programmer

---

## 📞 联系方式

- GitHub Issues: [提交问题](https://github.com/huiyijian/ai-fall-detection/issues)

---

*Last Updated: 2025-12-27*
